{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/DarkKnight1991/Stock-Price-Prediction/blob/master/stock_pred_main.py\n",
    "# https://towardsdatascience.com/predicting-stock-price-with-lstm-13af86a74944\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd \n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pickle\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "\n",
    "    df = pd.read_csv(\"./stockPrice.csv\", header=0, na_values='.')\n",
    "\n",
    "    lol = df.copy()\n",
    "    lol[\"1pred\"] = (df[\"close\"].shift(-1) - df[\"close\"]) / df[\"close\"]\n",
    "    lol[\"3pred\"] = (df[\"close\"].shift(-3) - df[\"close\"]) / df[\"close\"]\n",
    "    lol[\"5pred\"] = (df[\"close\"].shift(-5) - df[\"close\"]) / df[\"close\"]\n",
    "    lol[\"10pred\"] = (df[\"close\"].shift(-10) - df[\"close\"]) / df[\"close\"]\n",
    "\n",
    "    lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '1predB'] = 0\n",
    "    lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '1predB'] = 1\n",
    "    lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '3predB'] = 0\n",
    "    lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '3predB'] = 1\n",
    "    lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '5predB'] = 0\n",
    "    lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '5predB'] = 1\n",
    "    lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '10predB'] = 0\n",
    "    lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '10predB'] = 1\n",
    "\n",
    "    lol[\"5max\"] = lol[\"close\"].rolling(window=5).max()\n",
    "    lol[\"10max\"] = lol[\"close\"].rolling(window=10).max()\n",
    "    lol[\"20max\"] = lol[\"close\"].rolling(window=20).max()\n",
    "\n",
    "    lol[\"5low\"] = lol[\"close\"].rolling(window=5).min()\n",
    "    lol[\"10low\"] = lol[\"close\"].rolling(window=10).min()\n",
    "    lol[\"20low\"] = lol[\"close\"].rolling(window=20).min()\n",
    "\n",
    "    lol[\"vol%\"] =  (df[\"vol\"] - df[\"vol\"].shift(1)) /df[\"vol\"].shift(1)\n",
    "\n",
    "    lol = lol.dropna()    \n",
    "\n",
    "    dfC = pd.DataFrame()\n",
    "    dfC[\"vol\"] = lol[\"vol%\"]\n",
    "    dfC[\"sma10\"] = lol[\"sma10\"] / lol[\"close\"]\n",
    "    dfC[\"sma20\"] = lol[\"sma20\"] / lol[\"close\"]\n",
    "    dfC[\"sma50\"] = lol[\"sma50\"] / lol[\"close\"]\n",
    "    dfC[\"sma100\"] = lol[\"sma100\"] / lol[\"close\"]\n",
    "    dfC[\"vwap\"] = lol[\"vwap\"]\n",
    "    dfC[\"bbmid\"] = lol[\"bbmid\"] / lol[\"close\"]\n",
    "    dfC[\"bbUpper\"] = lol[\"bbUpper\"] / lol[\"close\"]\n",
    "    dfC[\"bbLower\"] = lol[\"bbLower\"] / lol[\"close\"]\n",
    "    dfC[\"cci\"] = lol[\"cci\"] \n",
    "    dfC[\"rsi\"] = lol[\"rsi\"] \n",
    "    dfC[\"5max\"] = lol[\"5max\"] / lol[\"close\"]\n",
    "    dfC[\"10max\"] = lol[\"10max\"] / lol[\"close\"]\n",
    "    dfC[\"20max\"] = lol[\"20max\"] / lol[\"close\"]\n",
    "    dfC[\"5low\"] = lol[\"5low\"] / lol[\"close\"]\n",
    "    dfC[\"10low\"] = lol[\"10low\"] / lol[\"close\"]\n",
    "    dfC[\"20low\"] = lol[\"20low\"] / lol[\"close\"]\n",
    "    dfC[\"1pred\"] = lol[\"1pred\"]\n",
    "    dfC[\"3pred\"] = lol[\"3pred\"]\n",
    "    dfC[\"5pred\"] = lol[\"5pred\"]\n",
    "    dfC[\"10pred\"] = lol[\"10pred\"]\n",
    "    dfC[\"1predB\"] = lol[\"1predB\"]\n",
    "    dfC[\"3predB\"] = lol[\"3predB\"]\n",
    "    dfC[\"5predB\"] = lol[\"5predB\"]\n",
    "    dfC[\"10predB\"] = lol[\"10predB\"]\n",
    "\n",
    "    dfC.replace([np.inf, -np.inf], np.nan)\n",
    "    dfC.dropna(inplace=True)\n",
    "\n",
    "    df = dfC[['1pred', 'sma10', 'sma20', 'sma50', 'sma100', 'vwap', 'bbmid', 'bbUpper', 'bbLower', 'cci', 'rsi', '5max', '10max', '20max', '5low', '10low', '20low']]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def min_max_scale(X, range=(0, 1)):\n",
    "# #     mi, ma = range\n",
    "# #     X_std = (X - X.min()) / (X.max() - X.min())\n",
    "# #     X_scaled = X_std * (ma - mi) + mi\n",
    "# #     return X_scaled\n",
    "\n",
    "# # print(min_max_scale(df))\n",
    "\n",
    "\n",
    "\n",
    "# df_train, df_test = train_test_split(df, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "# print(\"Train and Test size\", len(df_train), len(df_test))\n",
    "# # scale the feature MinMax, build array\n",
    "# x = df_train.loc[:,df_train.shape[0]].values\n",
    "# min_max_scaler = MinMaxScaler()\n",
    "# x_train = min_max_scaler.fit_transform(x)\n",
    "# x_test = min_max_scaler.transform(df_test.loc[:,df_train.shape[0]])\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": 20,  # 20<16<10, 25 was a bust\n",
    "    \"epochs\": 200,\n",
    "    \"lr\": 0.00010000,\n",
    "    \"time_steps\": 10\n",
    "}\n",
    "\n",
    "\n",
    "# INPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"/inputs\"\n",
    "# OUTPUT_PATH = PATH_TO_DRIVE_ML_DATA+\"/outputs/lstm_best_7-3-19_12AM/\"+iter_changes\n",
    "TIME_STEPS = params[\"time_steps\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "stime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_dataset(mat,batch_size):\n",
    "    \"\"\"\n",
    "    trims dataset to a size that's divisible by BATCH_SIZE\n",
    "    \"\"\"\n",
    "    no_of_rows_drop = mat.shape[0]%batch_size\n",
    "    if no_of_rows_drop > 0:\n",
    "        return mat[:-no_of_rows_drop]\n",
    "    else:\n",
    "        return mat\n",
    "\n",
    "\n",
    "def build_timeseries(mat, y_col_index):\n",
    "    \"\"\"\n",
    "    Converts ndarray into timeseries format and supervised data format. Takes first TIME_STEPS\n",
    "    number of rows as input and sets the TIME_STEPS+1th data as corresponding output and so on.\n",
    "    :param mat: ndarray which holds the dataset\n",
    "    :param y_col_index: index of column which acts as output\n",
    "    :return: returns two ndarrays-- input and output in format suitable to feed\n",
    "    to LSTM.\n",
    "    \"\"\"\n",
    "    # total number of time-series samples would be len(mat) - TIME_STEPS\n",
    "    dim_0 = mat.shape[0] - TIME_STEPS\n",
    "    dim_1 = mat.shape[1]\n",
    "    x = np.zeros((dim_0, TIME_STEPS, dim_1))\n",
    "    y = np.zeros((dim_0,))\n",
    "    print(\"dim_0\",dim_0)\n",
    "    for i in tqdm_notebook(range(dim_0)):\n",
    "        x[i] = mat[i:TIME_STEPS+i]\n",
    "        y[i] = mat[TIME_STEPS+i, y_col_index]\n",
    "#         if i < 10:\n",
    "#           print(i,\"-->\", x[i,-1,:], y[i])\n",
    "    print(\"length of time-series i/o\",x.shape,y.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2450, 17)\n",
      "Index(['1pred', 'sma10', 'sma20', 'sma50', 'sma100', 'vwap', 'bbmid',\n",
      "       'bbUpper', 'bbLower', 'cci', 'rsi', '5max', '10max', '20max', '5low',\n",
      "       '10low', '20low'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joaki\\.conda\\envs\\3.7\\lib\\site-packages\\tqdm\\_tqdm.py:605: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PanelGroupBy' from 'pandas.core.groupby' (C:\\Users\\joaki\\.conda\\envs\\3.7\\lib\\site-packages\\pandas\\core\\groupby\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36mpandas\u001b[1;34m(tclass, *targs, **tkwargs)\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[1;31m# pandas>=0.23.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 613\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrameGroupBy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    614\u001b[0m                 \u001b[0mSeriesGroupBy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGroupBy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPanelGroupBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DataFrameGroupBy' from 'pandas.core.groupby.groupby' (C:\\Users\\joaki\\.conda\\envs\\3.7\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-71a491f092ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_ge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# display(df_ge.head(5))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Processing...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_ge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtrain_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'1pred'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sma10'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sma20'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sma50'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sma100'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vwap'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bbmid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bbUpper'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'bbLower'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cci'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rsi'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'10max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'20max'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5low'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'10low'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'20low'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36mpandas\u001b[1;34m(tclass, *targs, **tkwargs)\u001b[0m\n\u001b[0;32m    614\u001b[0m                 \u001b[0mSeriesGroupBy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGroupBy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPanelGroupBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrameGroupBy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m                 \u001b[0mSeriesGroupBy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGroupBy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPanelGroupBy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PanelGroupBy' from 'pandas.core.groupby' (C:\\Users\\joaki\\.conda\\envs\\3.7\\lib\\site-packages\\pandas\\core\\groupby\\__init__.py)"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "df_ge = getData()\n",
    "\n",
    "# print(df_ge)\n",
    "\n",
    "print(df_ge.shape)\n",
    "print(df_ge.columns)\n",
    "# display(df_ge.head(5))\n",
    "# tqdm_notebook.pandas('Processing...')\n",
    "print(df_ge.dtypes)\n",
    "train_cols = ['1pred', 'sma10', 'sma20', 'sma50', 'sma100', 'vwap', 'bbmid', 'bbUpper', 'bbLower', 'cci', 'rsi', '5max', '10max', '20max', '5low', '10low', '20low']\n",
    "df_train, df_test = train_test_split(df_ge, train_size=0.8, test_size=0.2, shuffle=False)\n",
    "print(\"Train--Test size\", len(df_train), len(df_test))\n",
    "\n",
    "# scale the feature MinMax, build array\n",
    "x = df_train.loc[:,train_cols].values\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_train = min_max_scaler.fit_transform(x)\n",
    "x_test = min_max_scaler.transform(df_test.loc[:,train_cols])\n",
    "\n",
    "print(\"Deleting unused dataframes of total size(KB)\",(sys.getsizeof(df_ge)+sys.getsizeof(df_train)+sys.getsizeof(df_test))//1024)\n",
    "\n",
    "del df_ge\n",
    "del df_test\n",
    "del df_train\n",
    "del x\n",
    "\n",
    "print(\"Are any NaNs present in train/test matrices?\",np.isnan(x_train).any(), np.isnan(x_train).any())\n",
    "x_t, y_t = build_timeseries(x_train, 3)\n",
    "x_t = trim_dataset(x_t, BATCH_SIZE)\n",
    "y_t = trim_dataset(y_t, BATCH_SIZE)\n",
    "print(\"Batch trimmed size\",x_t.shape, y_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    lstm_model = Sequential()\n",
    "    # (batch_size, timesteps, data_dim)\n",
    "    lstm_model.add(LSTM(100, batch_input_shape=(BATCH_SIZE, TIME_STEPS, x_t.shape[2]),\n",
    "                        dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,\n",
    "                        kernel_initializer='random_uniform'))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(LSTM(60, dropout=0.0))\n",
    "    lstm_model.add(Dropout(0.4))\n",
    "    lstm_model.add(Dense(20,activation='relu'))\n",
    "    lstm_model.add(Dense(1,activation='sigmoid'))\n",
    "    optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    lstm_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_temp, y_temp = build_timeseries(x_test, 3)\n",
    "x_val, x_test_t = np.split(trim_dataset(x_temp, BATCH_SIZE),2)\n",
    "y_val, y_test_t = np.split(trim_dataset(y_temp, BATCH_SIZE),2)\n",
    "\n",
    "print(\"Test size\", x_test_t.shape, y_test_t.shape, x_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                   patience=40, min_delta=0.0001)\n",
    "\n",
    "# mcp = ModelCheckpoint(os.path.join(OUTPUT_PATH,\n",
    "#                       \"best_model.h5\"), monitor='val_loss', verbose=1,\n",
    "#                       save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
    "\n",
    "# Not used here. But leaving it here as a reminder for future\n",
    "r_lr_plat = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.1,\n",
    "                              patience=30, \n",
    "                              verbose=0,\n",
    "                              mode='auto',\n",
    "                              min_delta=0.0001,\n",
    "                              cooldown=0,\n",
    "                              min_lr=0)\n",
    "\n",
    "# csv_logger = CSVLogger(os.path.join(OUTPUT_PATH, 'training_log_' + time.ctime().replace(\" \",\"_\") + '.log'), append=True)\n",
    "\n",
    "history = model.fit(x_t, y_t,\n",
    "#                     epochs=30,\n",
    "                    epochs=params[\"epochs\"],\n",
    "                    verbose=2,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    validation_data=(trim_dataset(x_val, BATCH_SIZE),\n",
    "                    trim_dataset(y_val, BATCH_SIZE)),\n",
    "                    callbacks=[es])                    \n",
    "#                     callbacks=[es, mcp, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(trim_dataset(x_test_t, BATCH_SIZE), batch_size=BATCH_SIZE)\n",
    "y_pred = y_pred.flatten()\n",
    "y_test_t = trim_dataset(y_test_t, BATCH_SIZE)\n",
    "error = mean_squared_error(y_test_t, y_pred)\n",
    "print(\"Error is\", error, y_pred.shape, y_test_t.shape)\n",
    "print(y_pred[0:15])\n",
    "print(y_test_t[0:15])\n",
    "\n",
    "# convert the predicted value to range of real data\n",
    "y_pred_org = (y_pred * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# min_max_scaler.inverse_transform(y_pred)\n",
    "y_test_t_org = (y_test_t * min_max_scaler.data_range_[3]) + min_max_scaler.data_min_[3]\n",
    "# min_max_scaler.inverse_transform(y_test_t)\n",
    "print(y_pred_org[0:15])\n",
    "print(y_test_t_org[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(y_pred_org)\n",
    "plt.plot(y_test_t_org)\n",
    "plt.title('Prediction vs Real Stock Price')\n",
    "plt.ylabel('Price')\n",
    "plt.xlabel('Days')\n",
    "plt.legend(['Prediction', 'Real'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
