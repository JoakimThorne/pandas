{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/DarkKnight1991/Stock-Price-Prediction/blob/master/stock_pred_main.py\n",
    "# https://towardsdatascience.com/predicting-stock-price-with-lstm-13af86a74944\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd \n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "import pickle\n",
    "\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import logging\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS = 10\n",
    "test_set_size_percentage = 20 \n",
    "COLUMNS = ['close', 'sma10', 'sma20', 'sma50', 'sma100', 'vwap', 'bbmid', 'bbUpper', 'bbLower', 'cci', 'rsi', '5max', '10max', '20max', '5low', '10low', '20low']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, validation, test data given stock data and sequence length\n",
    "def load_data(stock, seq_len):\n",
    "    data_raw = stock.as_matrix() # convert to numpy array\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - seq_len): \n",
    "        data.append(data_raw[index: index + seq_len])\n",
    "    \n",
    "    data = np.array(data);\n",
    "    test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]));\n",
    "    train_set_size = data.shape[0] - (test_set_size);\n",
    "    \n",
    "    x_train = data[:train_set_size,:-1,:]\n",
    "    y_train = data[:train_set_size,-1,:]\n",
    "    \n",
    "    \n",
    "    x_test = data[train_set_size:,:-1,:]\n",
    "    y_test = data[train_set_size:,-1,:]\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(df):\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    for column in df:\n",
    "        df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    TIME_STEPS = 10\n",
    "    test_set_size_percentage = 20 \n",
    "    COLUMNS = ['close', 'sma10', 'sma20', 'sma50', 'sma100', 'vwap', 'bbmid', 'bbUpper', 'bbLower', 'cci', 'rsi', '5max', '10max', '20max', '5low', '10low', '20low']\n",
    "\n",
    "    df = pd.read_csv(\"./stockPrice.csv\", header=0, na_values='.')\n",
    "\n",
    "    lol = df.copy()\n",
    "    lol[\"1pred\"] = (df[\"close\"].shift(-1) - df[\"close\"]) / df[\"close\"]\n",
    "    lol[\"3pred\"] = (df[\"close\"].shift(-3) - df[\"close\"]) / df[\"close\"]\n",
    "    lol[\"5pred\"] = (df[\"close\"].shift(-5) - df[\"close\"]) / df[\"close\"]\n",
    "    lol[\"10pred\"] = (df[\"close\"].shift(-10) - df[\"close\"]) / df[\"close\"]\n",
    "\n",
    "    lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '1predB'] = 0\n",
    "    lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '1predB'] = 1\n",
    "    lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '3predB'] = 0\n",
    "    lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '3predB'] = 1\n",
    "    lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '5predB'] = 0\n",
    "    lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '5predB'] = 1\n",
    "    lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '10predB'] = 0\n",
    "    lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '10predB'] = 1\n",
    "\n",
    "    lol[\"5max\"] = lol[\"close\"].rolling(window=5).max()\n",
    "    lol[\"10max\"] = lol[\"close\"].rolling(window=10).max()\n",
    "    lol[\"20max\"] = lol[\"close\"].rolling(window=20).max()\n",
    "\n",
    "    lol[\"5low\"] = lol[\"close\"].rolling(window=5).min()\n",
    "    lol[\"10low\"] = lol[\"close\"].rolling(window=10).min()\n",
    "    lol[\"20low\"] = lol[\"close\"].rolling(window=20).min()\n",
    "\n",
    "    lol[\"vol%\"] =  (df[\"vol\"] - df[\"vol\"].shift(1)) /df[\"vol\"].shift(1)\n",
    "\n",
    "    lol = lol.dropna()    \n",
    "\n",
    "    dfC = pd.DataFrame()\n",
    "    dfC[\"vol\"] = lol[\"vol%\"]\n",
    "    dfC[\"sma10\"] = lol[\"sma10\"] / lol[\"close\"]\n",
    "    dfC[\"sma20\"] = lol[\"sma20\"] / lol[\"close\"]\n",
    "    dfC[\"sma50\"] = lol[\"sma50\"] / lol[\"close\"]\n",
    "    dfC[\"sma100\"] = lol[\"sma100\"] / lol[\"close\"]\n",
    "    dfC[\"vwap\"] = lol[\"vwap\"]\n",
    "    dfC[\"bbmid\"] = lol[\"bbmid\"] / lol[\"close\"]\n",
    "    dfC[\"bbUpper\"] = lol[\"bbUpper\"] / lol[\"close\"]\n",
    "    dfC[\"bbLower\"] = lol[\"bbLower\"] / lol[\"close\"]\n",
    "    dfC[\"cci\"] = lol[\"cci\"] \n",
    "    dfC[\"rsi\"] = lol[\"rsi\"] \n",
    "    dfC[\"5max\"] = lol[\"5max\"] / lol[\"close\"]\n",
    "    dfC[\"10max\"] = lol[\"10max\"] / lol[\"close\"]\n",
    "    dfC[\"20max\"] = lol[\"20max\"] / lol[\"close\"]\n",
    "    dfC[\"5low\"] = lol[\"5low\"] / lol[\"close\"]\n",
    "    dfC[\"10low\"] = lol[\"10low\"] / lol[\"close\"]\n",
    "    dfC[\"20low\"] = lol[\"20low\"] / lol[\"close\"]\n",
    "    dfC[\"1pred\"] = lol[\"1pred\"]\n",
    "    dfC[\"3pred\"] = lol[\"3pred\"]\n",
    "    dfC[\"5pred\"] = lol[\"5pred\"]\n",
    "    dfC[\"10pred\"] = lol[\"10pred\"]\n",
    "    dfC[\"1predB\"] = lol[\"1predB\"]\n",
    "    dfC[\"3predB\"] = lol[\"3predB\"]\n",
    "    dfC[\"5predB\"] = lol[\"5predB\"]\n",
    "    dfC[\"10predB\"] = lol[\"10predB\"]\n",
    "    dfC[\"close\"] = lol[\"close\"]\n",
    "\n",
    "    dfC.replace([np.inf, -np.inf], np.nan)\n",
    "    dfC.dropna(inplace=True)\n",
    "\n",
    "    df = dfC[COLUMNS]\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    for column in df:\n",
    "        df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
    "\n",
    "    \n",
    "    data_raw = df.as_matrix() # convert to numpy array\n",
    "    data = []\n",
    "\n",
    "    # create all possible sequences of length seq_len\n",
    "    for index in range(len(data_raw) - TIME_STEPS): \n",
    "        data.append(data_raw[index: index + TIME_STEPS])\n",
    "\n",
    "    data = np.array(data);\n",
    "    test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]));\n",
    "    train_set_size = data.shape[0] - (test_set_size);\n",
    "\n",
    "    x_train = data[:train_set_size,:-1,:]\n",
    "    y_train = data[:train_set_size,-1,:]\n",
    "\n",
    "\n",
    "    x_test = data[train_set_size:,:-1,:]\n",
    "    y_test = data[train_set_size:,-1,:]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Choose only close prices\n",
    "    # x_train, y_train, x_test, y_test\n",
    "    y_train = y_train[:,0]\n",
    "    y_test = y_test[:,0]\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model(x_train, y_train, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Model providing function:\n",
    "\n",
    "    Create Keras model with double curly brackets dropped-in as needed.\n",
    "    Return value has to be a valid python dictionary with two customary keys:\n",
    "        - loss: Specify a numeric evaluation metric to be minimized\n",
    "        - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "    The last one is optional, though recommended, namely:\n",
    "        - model: specify the model just created so that we can later use it again.\n",
    "    \"\"\"\n",
    "    input_dim = x_train.shape[2] - 1 # TODO Window is wrong # Number of features\n",
    "#     output_dim = y_train.shape[2]  # Number of features\n",
    "    print(input_dim)\n",
    "    TIME_STEPS = 10\n",
    "\n",
    "#     ''''''''''''''''''''''''''''''''''''''''''''''\n",
    "    model = Sequential()\n",
    "\n",
    " # (batch_size, timesteps, data_dim)\n",
    "    model.add(LSTM({{choice([256, 512, 1024])}}, batch_input_shape=(20, TIME_STEPS, input_dim),\n",
    "                        dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,\n",
    "                        kernel_initializer='random_uniform'))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(LSTM({{choice([256, 512, 1024])}}, return_sequences=False))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    \n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid','softmax'])}}))\n",
    "    model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "    model.add(Activation({{choice(['relu', 'sigmoid','softmax'])}}))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation({{choice(['sigmoid','softmax'])}}))\n",
    "    \n",
    "    \n",
    "#     optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#     model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "#     optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
    "    # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='mean_squared_error', optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
    "                       patience=40, min_delta=0.0001)\n",
    "                                                             \n",
    "    result = model.fit(x_train, y_train,\n",
    "             batch_size=1,\n",
    "             epochs=300,\n",
    "             callbacks=[es],\n",
    "             shuffle=False,\n",
    "             validation_split=0.1)\n",
    "                                                             \n",
    "    score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test accuracy:', acc)\n",
    "    return {'loss': -acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import sys\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tqdm._tqdm_notebook import tqdm_notebook\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pickle\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import MinMaxScaler\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import mean_squared_error\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import logging\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential, load_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import optimizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import layers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Activation, Dense, Dropout, LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'LSTM': hp.choice('LSTM', [256, 512, 1024]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'LSTM_1': hp.choice('LSTM_1', [256, 512, 1024]),\n",
      "        'Dropout_1': hp.uniform('Dropout_1', 0, 1),\n",
      "        'Dropout_2': hp.uniform('Dropout_2', 0, 1),\n",
      "        'LSTM_2': hp.choice('LSTM_2', [256, 512, 1024]),\n",
      "        'Activation': hp.choice('Activation', ['relu', 'sigmoid','softmax']),\n",
      "        'LSTM_3': hp.choice('LSTM_3', [256, 512, 1024]),\n",
      "        'Activation_1': hp.choice('Activation_1', ['relu', 'sigmoid','softmax']),\n",
      "        'Activation_2': hp.choice('Activation_2', ['sigmoid','softmax']),\n",
      "        'optimizer': hp.choice('optimizer', ['rmsprop', 'adam', 'sgd']),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: TIME_STEPS = 10\n",
      "   3: test_set_size_percentage = 20 \n",
      "   4: COLUMNS = ['close', 'sma10', 'sma20', 'sma50', 'sma100', 'vwap', 'bbmid', 'bbUpper', 'bbLower', 'cci', 'rsi', '5max', '10max', '20max', '5low', '10low', '20low']\n",
      "   5: \n",
      "   6: df = pd.read_csv(\"./stockPrice.csv\", header=0, na_values='.')\n",
      "   7: \n",
      "   8: lol = df.copy()\n",
      "   9: lol[\"1pred\"] = (df[\"close\"].shift(-1) - df[\"close\"]) / df[\"close\"]\n",
      "  10: lol[\"3pred\"] = (df[\"close\"].shift(-3) - df[\"close\"]) / df[\"close\"]\n",
      "  11: lol[\"5pred\"] = (df[\"close\"].shift(-5) - df[\"close\"]) / df[\"close\"]\n",
      "  12: lol[\"10pred\"] = (df[\"close\"].shift(-10) - df[\"close\"]) / df[\"close\"]\n",
      "  13: \n",
      "  14: lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '1predB'] = 0\n",
      "  15: lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '1predB'] = 1\n",
      "  16: lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '3predB'] = 0\n",
      "  17: lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '3predB'] = 1\n",
      "  18: lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '5predB'] = 0\n",
      "  19: lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '5predB'] = 1\n",
      "  20: lol.loc[df[\"close\"].shift(-1) > df[\"close\"] , '10predB'] = 0\n",
      "  21: lol.loc[df[\"close\"].shift(-1) < df[\"close\"] , '10predB'] = 1\n",
      "  22: \n",
      "  23: lol[\"5max\"] = lol[\"close\"].rolling(window=5).max()\n",
      "  24: lol[\"10max\"] = lol[\"close\"].rolling(window=10).max()\n",
      "  25: lol[\"20max\"] = lol[\"close\"].rolling(window=20).max()\n",
      "  26: \n",
      "  27: lol[\"5low\"] = lol[\"close\"].rolling(window=5).min()\n",
      "  28: lol[\"10low\"] = lol[\"close\"].rolling(window=10).min()\n",
      "  29: lol[\"20low\"] = lol[\"close\"].rolling(window=20).min()\n",
      "  30: \n",
      "  31: lol[\"vol%\"] =  (df[\"vol\"] - df[\"vol\"].shift(1)) /df[\"vol\"].shift(1)\n",
      "  32: \n",
      "  33: lol = lol.dropna()    \n",
      "  34: \n",
      "  35: dfC = pd.DataFrame()\n",
      "  36: dfC[\"vol\"] = lol[\"vol%\"]\n",
      "  37: dfC[\"sma10\"] = lol[\"sma10\"] / lol[\"close\"]\n",
      "  38: dfC[\"sma20\"] = lol[\"sma20\"] / lol[\"close\"]\n",
      "  39: dfC[\"sma50\"] = lol[\"sma50\"] / lol[\"close\"]\n",
      "  40: dfC[\"sma100\"] = lol[\"sma100\"] / lol[\"close\"]\n",
      "  41: dfC[\"vwap\"] = lol[\"vwap\"]\n",
      "  42: dfC[\"bbmid\"] = lol[\"bbmid\"] / lol[\"close\"]\n",
      "  43: dfC[\"bbUpper\"] = lol[\"bbUpper\"] / lol[\"close\"]\n",
      "  44: dfC[\"bbLower\"] = lol[\"bbLower\"] / lol[\"close\"]\n",
      "  45: dfC[\"cci\"] = lol[\"cci\"] \n",
      "  46: dfC[\"rsi\"] = lol[\"rsi\"] \n",
      "  47: dfC[\"5max\"] = lol[\"5max\"] / lol[\"close\"]\n",
      "  48: dfC[\"10max\"] = lol[\"10max\"] / lol[\"close\"]\n",
      "  49: dfC[\"20max\"] = lol[\"20max\"] / lol[\"close\"]\n",
      "  50: dfC[\"5low\"] = lol[\"5low\"] / lol[\"close\"]\n",
      "  51: dfC[\"10low\"] = lol[\"10low\"] / lol[\"close\"]\n",
      "  52: dfC[\"20low\"] = lol[\"20low\"] / lol[\"close\"]\n",
      "  53: dfC[\"1pred\"] = lol[\"1pred\"]\n",
      "  54: dfC[\"3pred\"] = lol[\"3pred\"]\n",
      "  55: dfC[\"5pred\"] = lol[\"5pred\"]\n",
      "  56: dfC[\"10pred\"] = lol[\"10pred\"]\n",
      "  57: dfC[\"1predB\"] = lol[\"1predB\"]\n",
      "  58: dfC[\"3predB\"] = lol[\"3predB\"]\n",
      "  59: dfC[\"5predB\"] = lol[\"5predB\"]\n",
      "  60: dfC[\"10predB\"] = lol[\"10predB\"]\n",
      "  61: dfC[\"close\"] = lol[\"close\"]\n",
      "  62: \n",
      "  63: dfC.replace([np.inf, -np.inf], np.nan)\n",
      "  64: dfC.dropna(inplace=True)\n",
      "  65: \n",
      "  66: df = dfC[COLUMNS]\n",
      "  67: min_max_scaler = MinMaxScaler()\n",
      "  68: for column in df:\n",
      "  69:     df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "  70: \n",
      "  71: \n",
      "  72: data_raw = df.as_matrix() # convert to numpy array\n",
      "  73: data = []\n",
      "  74: \n",
      "  75: # create all possible sequences of length seq_len\n",
      "  76: for index in range(len(data_raw) - TIME_STEPS): \n",
      "  77:     data.append(data_raw[index: index + TIME_STEPS])\n",
      "  78: \n",
      "  79: data = np.array(data);\n",
      "  80: test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]));\n",
      "  81: train_set_size = data.shape[0] - (test_set_size);\n",
      "  82: \n",
      "  83: x_train = data[:train_set_size,:-1,:]\n",
      "  84: y_train = data[:train_set_size,-1,:]\n",
      "  85: \n",
      "  86: \n",
      "  87: x_test = data[train_set_size:,:-1,:]\n",
      "  88: y_test = data[train_set_size:,-1,:]\n",
      "  89: \n",
      "  90: \n",
      "  91: \n",
      "  92: # Choose only close prices\n",
      "  93: # x_train, y_train, x_test, y_test\n",
      "  94: y_train = y_train[:,0]\n",
      "  95: y_test = y_test[:,0]\n",
      "  96: \n",
      "  97: \n",
      "  98: \n",
      "  99: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     \"\"\"\n",
      "   4:     Model providing function:\n",
      "   5: \n",
      "   6:     Create Keras model with double curly brackets dropped-in as needed.\n",
      "   7:     Return value has to be a valid python dictionary with two customary keys:\n",
      "   8:         - loss: Specify a numeric evaluation metric to be minimized\n",
      "   9:         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
      "  10:     The last one is optional, though recommended, namely:\n",
      "  11:         - model: specify the model just created so that we can later use it again.\n",
      "  12:     \"\"\"\n",
      "  13:     input_dim = x_train.shape[2] - 1 # TODO Window is wrong # Number of features\n",
      "  14: #     output_dim = y_train.shape[2]  # Number of features\n",
      "  15:     print(input_dim)\n",
      "  16:     TIME_STEPS = 10\n",
      "  17: \n",
      "  18: #     ''''''''''''''''''''''''''''''''''''''''''''''\n",
      "  19:     model = Sequential()\n",
      "  20: \n",
      "  21:  # (batch_size, timesteps, data_dim)\n",
      "  22:     model.add(LSTM(space['LSTM'], batch_input_shape=(20, TIME_STEPS, input_dim),\n",
      "  23:                         dropout=0.0, recurrent_dropout=0.0, stateful=True, return_sequences=True,\n",
      "  24:                         kernel_initializer='random_uniform'))\n",
      "  25:     model.add(Dropout(space['Dropout']))\n",
      "  26:     model.add(LSTM(space['LSTM_1'], return_sequences=False))\n",
      "  27:     model.add(Dropout(space['Dropout_1']))\n",
      "  28:     \n",
      "  29:     model.add(Dropout(space['Dropout_2']))\n",
      "  30:     model.add(Dense(space['LSTM_2']))\n",
      "  31:     model.add(Activation(space['Activation']))\n",
      "  32:     model.add(Dense(space['LSTM_3']))\n",
      "  33:     model.add(Activation(space['Activation_1']))\n",
      "  34: \n",
      "  35:     model.add(Dense(1))\n",
      "  36:     model.add(Activation(space['Activation_2']))\n",
      "  37:     \n",
      "  38:     \n",
      "  39: #     optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
      "  40:     # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
      "  41: #     model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
      "  42:     \n",
      "  43: #     optimizer = optimizers.RMSprop(lr=params[\"lr\"])\n",
      "  44:     # optimizer = optimizers.SGD(lr=0.000001, decay=1e-6, momentum=0.9, nesterov=True)\n",
      "  45:     model.compile(loss='mean_squared_error', optimizer=space['optimizer'])\n",
      "  46: \n",
      "  47:     es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,\n",
      "  48:                        patience=40, min_delta=0.0001)\n",
      "  49:                                                              \n",
      "  50:     result = model.fit(x_train, y_train,\n",
      "  51:              batch_size=1,\n",
      "  52:              epochs=300,\n",
      "  53:              callbacks=[es],\n",
      "  54:              shuffle=False,\n",
      "  55:              validation_split=0.1)\n",
      "  56:                                                              \n",
      "  57:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  58:     print('Test accuracy:', acc)\n",
      "  59:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  60: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:171: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df[column] = min_max_scaler.fit_transform(df[column].values.reshape(-1,1))\n",
      "C:\\Users\\joaki\\jupyter\\kpi\\temp_model.py:174: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  data_raw = df.as_matrix() # convert to numpy array\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16                                                                                                                     \n",
      "  0%|                                                                             | 0/10 [00:00<?, ?it/s, best loss: ?]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_13_input to have shape (10, 16) but got array with shape (9, 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-469-5d4c7c204cf8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                       \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                       \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'kpiLstmHyperas'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                                       trials=Trials())\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(model, data, algo, max_evals, trials, functions, rseed, notebook_name, verbose, eval_space, return_space, keep_temp)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                      \u001b[0mnotebook_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnotebook_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                                      \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                                      keep_temp=keep_temp)\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\hyperas\\optim.py\u001b[0m in \u001b[0;36mbase_minimizer\u001b[1;34m(model, data, functions, algo, max_evals, trials, rseed, full_model_string, notebook_name, verbose, stack, keep_temp)\u001b[0m\n\u001b[0;32m    137\u001b[0m              \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m              \u001b[0mrstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m              return_argmin=True),\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mget_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     )\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         )\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[0;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    225\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                         \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 844\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\jupyter\\kpi\\temp_model.py\u001b[0m in \u001b[0;36mkeras_fmin_fnct\u001b[1;34m(space)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\3.7\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_13_input to have shape (10, 16) but got array with shape (9, 17)"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                      data=getData,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=10,\n",
    "                                      notebook_name='kpiLstmHyperas',\n",
    "                                      trials=Trials())\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = getData()\n",
    "print(Y_train)\n",
    "print(Y_train.shape[2])\n",
    "\n",
    "print(\"Evalutation of best performing model:\")\n",
    "# print(best_model.evaluate(X_test, Y_test))\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "# print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(x_train, y_train, x_test, y_test):\n",
    "#     \"\"\"\n",
    "#     Model providing function:\n",
    "\n",
    "#     Create Keras model with double curly brackets dropped-in as needed.\n",
    "#     Return value has to be a valid python dictionary with two customary keys:\n",
    "#         - loss: Specify a numeric evaluation metric to be minimized\n",
    "#         - status: Just use STATUS_OK and see hyperopt documentation if not feasible\n",
    "#     The last one is optional, though recommended, namely:\n",
    "#         - model: specify the model just created so that we can later use it again.\n",
    "#     \"\"\"\n",
    "#     input_dim = x_train.shape[1]  # Number of features\n",
    "#     output_dim = y_train.shape[1]  # Number of features\n",
    "#     print(input_dim)\n",
    "\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Dense(512, input_dim=input_dim))\n",
    "#     model.add(Activation({{choice(['relu', 'sigmoid','softmax'])}}))\n",
    "#     model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "#     model.add(Activation({{choice(['relu', 'sigmoid','softmax'])}}))\n",
    "#     model.add(Dense({{choice([256, 512, 1024])}}))\n",
    "#     model.add(Activation({{choice(['relu', 'sigmoid','softmax'])}}))\n",
    "\n",
    "#     # If we choose 'four', add an additional fourth layer\n",
    "#     if {{choice(['three', 'four'])}} == 'four':\n",
    "#         model.add(Dense(100))\n",
    "#         # We can also choose between complete sets of layers\n",
    "#         model.add({{choice([Dropout(0.5), Activation('linear')])}})\n",
    "#         model.add(Activation({{choice(['relu', 'sigmoid','softmax'])}}))\n",
    "        \n",
    "#     model.add(Dense(output_dim))\n",
    "#     model.add(Activation('sigmoid'))\n",
    "\n",
    "#     model.compile(loss='binary_crossentropy', metrics=['accuracy'],\n",
    "#                   optimizer={{choice(['rmsprop', 'adam', 'sgd'])}})\n",
    "\n",
    "#     result = model.fit(x_train, y_train,\n",
    "#              batch_size={{choice([16, 32, 64])}},\n",
    "#              epochs=100,\n",
    "#              validation_data=(x_test, y_test))\n",
    "\n",
    "#     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "#     print('Test accuracy:', acc)\n",
    "#     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "\n",
    "\n",
    "# best_run, best_model = optim.minimize(model=create_model,\n",
    "#                                       data=data,\n",
    "#                                       algo=tpe.suggest,\n",
    "#                                       max_evals=10,\n",
    "#                                       notebook_name='kpiLstmHyperas',\n",
    "#                                       trials=Trials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
